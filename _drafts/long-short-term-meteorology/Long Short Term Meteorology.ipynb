{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been trying to learn about Recurrent Neural Networks (RNNs) for time series prediction at work. To that end I figured I need to get my feet wet with a personal project to really solidify my learning. This project will attempt to make predictions for the volume of rain on an hourly basis in Calgary, Aberta. The data I have comes from Alberta Agriculture and Forestry: https://agriculture.alberta.ca/acis/alberta-weather-data-viewer.jsp. The data are for the Calgary Int'L CS station at hourly intervals from September 2008 through to end of June 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "for csv_file in os.listdir(DATA_DIR):\n",
    "    df = pd.read_csv(DATA_DIR + csv_file, encoding='latin1')\n",
    "    df['Date (Local Standard Time)'] = pd.to_datetime(df['Date (Local Standard Time)'], \n",
    "                                                      infer_datetime_format=True)\n",
    "    raw_data = raw_data.append(df, ignore_index=True)\n",
    "\n",
    "raw_data = raw_data.sort_values('Date (Local Standard Time)').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns are not useful for this problem. For example 'Precip. Accumulated (mm)' measures the total cumulative precipitation by hour across the two months of each csv file. I will drop it and let the RNN work off of the hourly non cumulative precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.drop('Precip. Accumulated (mm)', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all variables are present back to September 2008 when the hourly measurements were first made available. We can see when different variables first appeared in the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station Name                       Calgary Int'L CS\n",
       "Date (Local Standard Time)      2016-05-01 00:00:00\n",
       "Precip. (mm)                                      0\n",
       "Air Temp. Inst. (°C)                          5.031\n",
       "Air Temp. Max. (°C)                           6.697\n",
       "Air Temp. Min. (°C)                           4.773\n",
       "Humidity Inst. (%)                            66.43\n",
       "Humidity Ave. (%)                               NaN\n",
       "Wind Speed 10 m Inst. (km/h)                  5.029\n",
       "Wind Dir. 10 m Inst. (°)                      199.4\n",
       "Wind Speed 10 m Ave. (km/h)                    3.42\n",
       "Wind Dir. 10 m Ave. (°)                       228.7\n",
       "Air Temp. Ave. (°C)                           6.018\n",
       "Incoming Solar Rad. (W/m2)                        0\n",
       "Peak Wind Speed 10 m (km/h)                   5.785\n",
       "Wind Dir. Peak 10 m (°)                       209.2\n",
       "Snow Depth (cm)                               0.274\n",
       "Est. Dew Point Temp. (°C)                     -0.03\n",
       "Wind Chill (°C)                               5.031\n",
       "Humidex (°C)                                  5.031\n",
       "Name: 67168, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_full_row = data.apply(lambda col: col.first_valid_index()).max()\n",
    "data.loc[first_full_row,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row when all of the values in each column are populated is May 1st 2016, and even then 'Humidity Ave. (%)' is never populated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18982, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('Humidity Ave. (%)', axis=1)\n",
    "data.loc[first_full_row:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with almost 19,000 rows of data with 15 complete predictors. If we want more predictors we might consider dropping addtional variables. Let's see when each column first shows up in the data and decide if this makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Station Name</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date (Local Standard Time)</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precip. (mm)</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Temp. Inst. (°C)</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Temp. Max. (°C)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Temp. Min. (°C)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidity Inst. (%)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind Speed 10 m Inst. (km/h)</th>\n",
       "      <td>66064</td>\n",
       "      <td>2016-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind Dir. 10 m Inst. (°)</th>\n",
       "      <td>66112</td>\n",
       "      <td>2016-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind Speed 10 m Ave. (km/h)</th>\n",
       "      <td>66064</td>\n",
       "      <td>2016-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind Dir. 10 m Ave. (°)</th>\n",
       "      <td>66112</td>\n",
       "      <td>2016-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Temp. Ave. (°C)</th>\n",
       "      <td>43531</td>\n",
       "      <td>2013-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incoming Solar Rad. (W/m2)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peak Wind Speed 10 m (km/h)</th>\n",
       "      <td>66112</td>\n",
       "      <td>2016-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind Dir. Peak 10 m (°)</th>\n",
       "      <td>67168</td>\n",
       "      <td>2016-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow Depth (cm)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Est. Dew Point Temp. (°C)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind Chill (°C)</th>\n",
       "      <td>66064</td>\n",
       "      <td>2016-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidex (°C)</th>\n",
       "      <td>18166</td>\n",
       "      <td>2010-09-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                row       date\n",
       "Station Name                      0 2008-09-01\n",
       "Date (Local Standard Time)        0 2008-09-01\n",
       "Precip. (mm)                      0 2008-09-01\n",
       "Air Temp. Inst. (°C)              0 2008-09-01\n",
       "Air Temp. Max. (°C)           18166 2010-09-28\n",
       "Air Temp. Min. (°C)           18166 2010-09-28\n",
       "Humidity Inst. (%)            18166 2010-09-28\n",
       "Wind Speed 10 m Inst. (km/h)  66064 2016-03-16\n",
       "Wind Dir. 10 m Inst. (°)      66112 2016-03-18\n",
       "Wind Speed 10 m Ave. (km/h)   66064 2016-03-16\n",
       "Wind Dir. 10 m Ave. (°)       66112 2016-03-18\n",
       "Air Temp. Ave. (°C)           43531 2013-08-20\n",
       "Incoming Solar Rad. (W/m2)    18166 2010-09-28\n",
       "Peak Wind Speed 10 m (km/h)   66112 2016-03-18\n",
       "Wind Dir. Peak 10 m (°)       67168 2016-05-01\n",
       "Snow Depth (cm)               18166 2010-09-28\n",
       "Est. Dew Point Temp. (°C)     18166 2010-09-28\n",
       "Wind Chill (°C)               66064 2016-03-16\n",
       "Humidex (°C)                  18166 2010-09-28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_non_null_by_row = pd.DataFrame(data.apply(lambda col: col.first_valid_index()))\n",
    "first_non_null_by_row.columns = ['row']\n",
    "first_non_null_by_row['date'] = first_non_null_by_row['row'].apply(\n",
    "    lambda row: data['Date (Local Standard Time)'][int(row)])\n",
    "first_non_null_by_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extend our complete data back to mid March by dropping 'Wind Dir. Peak 10 m (°)' or we could extend it as far as 2010-09-28 if we drop all of the wind realted variables includng wind chill. I think losing the wind related variables is too big of a loss to justify the additional data gained. Dropping the 'Wind Dir. Peak 10 m (°)' variable to get an extra month and a half of data might be worth while, but it's impossible to be sure at this stage. For the time being I will proceed with the data from May 1st 2016 onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data = data.loc[first_full_row:,'Date (Local Standard Time)':].copy()\n",
    "rnn_data['Precip. Binary'] = rnn_data['Precip. (mm)'] > 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date (Local Standard Time)</th>\n",
       "      <th>Precip. (mm)</th>\n",
       "      <th>Air Temp. Inst. (°C)</th>\n",
       "      <th>Air Temp. Max. (°C)</th>\n",
       "      <th>Air Temp. Min. (°C)</th>\n",
       "      <th>Humidity Inst. (%)</th>\n",
       "      <th>Wind Speed 10 m Inst. (km/h)</th>\n",
       "      <th>Wind Dir. 10 m Inst. (°)</th>\n",
       "      <th>Wind Speed 10 m Ave. (km/h)</th>\n",
       "      <th>Wind Dir. 10 m Ave. (°)</th>\n",
       "      <th>Air Temp. Ave. (°C)</th>\n",
       "      <th>Incoming Solar Rad. (W/m2)</th>\n",
       "      <th>Peak Wind Speed 10 m (km/h)</th>\n",
       "      <th>Wind Dir. Peak 10 m (°)</th>\n",
       "      <th>Snow Depth (cm)</th>\n",
       "      <th>Est. Dew Point Temp. (°C)</th>\n",
       "      <th>Wind Chill (°C)</th>\n",
       "      <th>Humidex (°C)</th>\n",
       "      <th>Precip. Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67168</th>\n",
       "      <td>2016-05-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.031</td>\n",
       "      <td>6.697</td>\n",
       "      <td>4.773</td>\n",
       "      <td>66.43</td>\n",
       "      <td>5.029</td>\n",
       "      <td>199.4</td>\n",
       "      <td>3.420</td>\n",
       "      <td>228.7</td>\n",
       "      <td>6.018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.785</td>\n",
       "      <td>209.2</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>5.031</td>\n",
       "      <td>5.031</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67169</th>\n",
       "      <td>2016-05-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.540</td>\n",
       "      <td>5.480</td>\n",
       "      <td>3.498</td>\n",
       "      <td>69.89</td>\n",
       "      <td>0.875</td>\n",
       "      <td>221.9</td>\n",
       "      <td>3.679</td>\n",
       "      <td>220.2</td>\n",
       "      <td>4.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.196</td>\n",
       "      <td>224.1</td>\n",
       "      <td>0.619</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>4.540</td>\n",
       "      <td>4.540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67170</th>\n",
       "      <td>2016-05-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.620</td>\n",
       "      <td>4.565</td>\n",
       "      <td>2.336</td>\n",
       "      <td>77.55</td>\n",
       "      <td>5.094</td>\n",
       "      <td>252.5</td>\n",
       "      <td>3.629</td>\n",
       "      <td>240.3</td>\n",
       "      <td>3.111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.693</td>\n",
       "      <td>248.1</td>\n",
       "      <td>0.775</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.620</td>\n",
       "      <td>2.620</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67171</th>\n",
       "      <td>2016-05-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.721</td>\n",
       "      <td>2.629</td>\n",
       "      <td>0.447</td>\n",
       "      <td>74.99</td>\n",
       "      <td>6.185</td>\n",
       "      <td>304.8</td>\n",
       "      <td>5.915</td>\n",
       "      <td>302.1</td>\n",
       "      <td>1.275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.796</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-2.409</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.721</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67172</th>\n",
       "      <td>2016-05-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.149</td>\n",
       "      <td>80.80</td>\n",
       "      <td>1.580</td>\n",
       "      <td>287.1</td>\n",
       "      <td>4.655</td>\n",
       "      <td>311.3</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.748</td>\n",
       "      <td>327.2</td>\n",
       "      <td>0.916</td>\n",
       "      <td>-2.349</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date (Local Standard Time)  Precip. (mm)  Air Temp. Inst. (°C)  \\\n",
       "67168        2016-05-01 00:00:00           0.0                 5.031   \n",
       "67169        2016-05-01 01:00:00           0.0                 4.540   \n",
       "67170        2016-05-01 02:00:00           0.0                 2.620   \n",
       "67171        2016-05-01 03:00:00           0.0                 0.721   \n",
       "67172        2016-05-01 04:00:00           0.0                 0.250   \n",
       "\n",
       "       Air Temp. Max. (°C)  Air Temp. Min. (°C)  Humidity Inst. (%)  \\\n",
       "67168                6.697                4.773               66.43   \n",
       "67169                5.480                3.498               69.89   \n",
       "67170                4.565                2.336               77.55   \n",
       "67171                2.629                0.447               74.99   \n",
       "67172                0.977                0.149               80.80   \n",
       "\n",
       "       Wind Speed 10 m Inst. (km/h)  Wind Dir. 10 m Inst. (°)  \\\n",
       "67168                         5.029                     199.4   \n",
       "67169                         0.875                     221.9   \n",
       "67170                         5.094                     252.5   \n",
       "67171                         6.185                     304.8   \n",
       "67172                         1.580                     287.1   \n",
       "\n",
       "       Wind Speed 10 m Ave. (km/h)  Wind Dir. 10 m Ave. (°)  \\\n",
       "67168                        3.420                    228.7   \n",
       "67169                        3.679                    220.2   \n",
       "67170                        3.629                    240.3   \n",
       "67171                        5.915                    302.1   \n",
       "67172                        4.655                    311.3   \n",
       "\n",
       "       Air Temp. Ave. (°C)  Incoming Solar Rad. (W/m2)  \\\n",
       "67168                6.018                         0.0   \n",
       "67169                4.438                         0.0   \n",
       "67170                3.111                         0.0   \n",
       "67171                1.275                         0.0   \n",
       "67172                0.570                         0.0   \n",
       "\n",
       "       Peak Wind Speed 10 m (km/h)  Wind Dir. Peak 10 m (°) Snow Depth (cm)  \\\n",
       "67168                        5.785                    209.2           0.274   \n",
       "67169                        7.196                    224.1           0.619   \n",
       "67170                        7.693                    248.1           0.775   \n",
       "67171                       10.796                    328.0           0.705   \n",
       "67172                        8.748                    327.2           0.916   \n",
       "\n",
       "       Est. Dew Point Temp. (°C)  Wind Chill (°C)  Humidex (°C)  \\\n",
       "67168                     -0.030            5.031         5.031   \n",
       "67169                     -0.524            4.540         4.540   \n",
       "67170                     -0.098            2.620         2.620   \n",
       "67171                     -2.409            0.721         0.721   \n",
       "67172                     -2.349            0.250         0.250   \n",
       "\n",
       "       Precip. Binary  \n",
       "67168           False  \n",
       "67169           False  \n",
       "67170           False  \n",
       "67171           False  \n",
       "67172           False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#scale_cols = rnn_data.columns[2:17]\n",
    "\n",
    "#for col in scale_cols:\n",
    "#    null_index = rnn_data[col].isnull()\n",
    "#    rnn_data.loc[~null_index, [col]] = scaler.fit_transform(rnn_data.loc[~null_index, [col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date (Local Standard Time)         0\n",
       "Precip. (mm)                       0\n",
       "Air Temp. Inst. (°C)               0\n",
       "Air Temp. Max. (°C)                0\n",
       "Air Temp. Min. (°C)                0\n",
       "Humidity Inst. (%)                 0\n",
       "Wind Speed 10 m Inst. (km/h)       0\n",
       "Wind Dir. 10 m Inst. (°)           2\n",
       "Wind Speed 10 m Ave. (km/h)        0\n",
       "Wind Dir. 10 m Ave. (°)            2\n",
       "Air Temp. Ave. (°C)                0\n",
       "Incoming Solar Rad. (W/m2)         1\n",
       "Peak Wind Speed 10 m (km/h)        2\n",
       "Wind Dir. Peak 10 m (°)            2\n",
       "Snow Depth (cm)                 1951\n",
       "Est. Dew Point Temp. (°C)          0\n",
       "Wind Chill (°C)                    0\n",
       "Humidex (°C)                       0\n",
       "Precip. Binary                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(rnn_data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the RNN, we need some way to evaluate it on data it hasn't seen before. Since I am treating this as a timeseries problem, it is necessary to do this while maintaining the sequantial nature of the data. To that end I will use a strategy called rolling origin resampling. I plan to feed the network 12 weeks of data, and then predict hourly precipitation for the following week. I will then skip two weeks forward from the previous origin to start the next rollng window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n"
     ]
    }
   ],
   "source": [
    "precip = rnn_data['Precip. (mm)'].copy()\n",
    "max_run = 0\n",
    "\n",
    "for i, g in precip.groupby([(precip != precip.shift()).cumsum()]):\n",
    "    if len(g.tolist()) > max_run:\n",
    "        max_run = len(g.tolist())\n",
    "        \n",
    "print(max_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_list = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "target_list = [18]\n",
    "\n",
    "#train_X, test_X, train_Y, test_Y = rolling_origin_resample(rnn_data, \n",
    "#                                                           X_cols = predictor_list, \n",
    "#                                                           Y_cols = target_list)\n",
    "\n",
    "# Model inputs\n",
    "#lag_setting = test_period\n",
    "batch_size = 168\n",
    "lookback = 1\n",
    "epochs = 100\n",
    "\n",
    "train_len = 101 * batch_size\n",
    "test_len = (rnn_data.shape[0] - train_len) // batch_size * batch_size\n",
    "\n",
    "train_X = rnn_data.iloc[:train_len, predictor_list].values.reshape(train_len,1,16)\n",
    "test_X = rnn_data.iloc[train_len:train_len+test_len, predictor_list].values.reshape(test_len,1,16)\n",
    "train_Y = rnn_data.iloc[:train_len, target_list].values\n",
    "test_Y = rnn_data.iloc[train_len:train_len+test_len, target_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date (Local Standard Time)      2018-05-21 02:00:00\n",
       "Precip. (mm)                                      0\n",
       "Air Temp. Inst. (°C)                          10.61\n",
       "Air Temp. Max. (°C)                           12.65\n",
       "Air Temp. Min. (°C)                           10.11\n",
       "Humidity Inst. (%)                            53.42\n",
       "Wind Speed 10 m Inst. (km/h)                 11.293\n",
       "Wind Dir. 10 m Inst. (°)                      316.2\n",
       "Wind Speed 10 m Ave. (km/h)                  12.226\n",
       "Wind Dir. 10 m Ave. (°)                       318.2\n",
       "Air Temp. Ave. (°C)                           11.62\n",
       "Incoming Solar Rad. (W/m2)                        0\n",
       "Peak Wind Speed 10 m (km/h)                  17.993\n",
       "Wind Dir. Peak 10 m (°)                       322.6\n",
       "Snow Depth (cm)                                   0\n",
       "Est. Dew Point Temp. (°C)                     2.252\n",
       "Wind Chill (°C)                               10.61\n",
       "Humidex (°C)                                  10.61\n",
       "Precip. Binary                                False\n",
       "Name: 85168, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_data.iloc[18000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/everett/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = (batch_size, lookback, train_X.shape[2])\n",
    "output_shape = train_Y.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,\n",
    "               batch_input_shape=batch_shape,\n",
    "               return_sequences=True,\n",
    "               stateful=True,\n",
    "               name=\"LSTM_1\"))\n",
    "model.add(LSTM(50, return_sequences=False, stateful=True, name=\"LSTM_2\"))\n",
    "model.add(Dense(1, name=\"Dense_3\", activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1848, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rnn_data['Precip. (mm)'] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16968 samples, validate on 1848 samples\n",
      "Epoch 1/100\n",
      "16968/16968 [==============================] - 1s 78us/step - loss: 0.2985 - val_loss: 0.2518\n",
      "Epoch 2/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2355 - val_loss: 0.2470\n",
      "Epoch 3/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2282 - val_loss: 0.2506\n",
      "Epoch 4/100\n",
      "16968/16968 [==============================] - 1s 45us/step - loss: 0.2240 - val_loss: 0.2413\n",
      "Epoch 5/100\n",
      "16968/16968 [==============================] - 1s 44us/step - loss: 0.2179 - val_loss: 0.2314\n",
      "Epoch 6/100\n",
      "16968/16968 [==============================] - 1s 42us/step - loss: 0.2156 - val_loss: 0.2225\n",
      "Epoch 7/100\n",
      "16968/16968 [==============================] - 1s 59us/step - loss: 0.2129 - val_loss: 0.2213\n",
      "Epoch 8/100\n",
      "16968/16968 [==============================] - 1s 44us/step - loss: 0.2112 - val_loss: 0.2208\n",
      "Epoch 9/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2098 - val_loss: 0.2126\n",
      "Epoch 10/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2059 - val_loss: 0.2057\n",
      "Epoch 11/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2035 - val_loss: 0.2118\n",
      "Epoch 12/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2060 - val_loss: 0.2095\n",
      "Epoch 13/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2053 - val_loss: 0.2017\n",
      "Epoch 14/100\n",
      "16968/16968 [==============================] - 1s 38us/step - loss: 0.2038 - val_loss: 0.2011\n",
      "Epoch 15/100\n",
      "16968/16968 [==============================] - 1s 39us/step - loss: 0.2001 - val_loss: 0.1966\n",
      "Epoch 16/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.2002 - val_loss: 0.2005\n",
      "Epoch 17/100\n",
      "16968/16968 [==============================] - 1s 56us/step - loss: 0.2013 - val_loss: 0.2031\n",
      "Epoch 18/100\n",
      "16968/16968 [==============================] - 1s 56us/step - loss: 0.1997 - val_loss: 0.2015\n",
      "Epoch 19/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1985 - val_loss: 0.2015\n",
      "Epoch 20/100\n",
      "16968/16968 [==============================] - 1s 53us/step - loss: 0.1992 - val_loss: 0.2066\n",
      "Epoch 21/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1996 - val_loss: 0.2107\n",
      "Epoch 22/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1955 - val_loss: 0.2013\n",
      "Epoch 23/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1949 - val_loss: 0.1966\n",
      "Epoch 24/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1932 - val_loss: 0.1895\n",
      "Epoch 25/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1903 - val_loss: 0.1843\n",
      "Epoch 26/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1897 - val_loss: 0.1971\n",
      "Epoch 27/100\n",
      "16968/16968 [==============================] - 1s 53us/step - loss: 0.1905 - val_loss: 0.1900\n",
      "Epoch 28/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1898 - val_loss: 0.1894\n",
      "Epoch 29/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1903 - val_loss: 0.1870\n",
      "Epoch 30/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1917 - val_loss: 0.1949\n",
      "Epoch 31/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1866 - val_loss: 0.1888\n",
      "Epoch 32/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1848 - val_loss: 0.1812\n",
      "Epoch 33/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1843 - val_loss: 0.1867\n",
      "Epoch 34/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1841 - val_loss: 0.1841\n",
      "Epoch 35/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1844 - val_loss: 0.1860\n",
      "Epoch 36/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1845 - val_loss: 0.1775\n",
      "Epoch 37/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1841 - val_loss: 0.1801\n",
      "Epoch 38/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1840 - val_loss: 0.1745\n",
      "Epoch 39/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1784 - val_loss: 0.1829\n",
      "Epoch 40/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1774 - val_loss: 0.1737\n",
      "Epoch 41/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1781 - val_loss: 0.1808\n",
      "Epoch 42/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1777 - val_loss: 0.1770\n",
      "Epoch 43/100\n",
      "16968/16968 [==============================] - 1s 59us/step - loss: 0.1795 - val_loss: 0.1843\n",
      "Epoch 44/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1785 - val_loss: 0.1841\n",
      "Epoch 45/100\n",
      "16968/16968 [==============================] - 1s 56us/step - loss: 0.1791 - val_loss: 0.1772\n",
      "Epoch 46/100\n",
      "16968/16968 [==============================] - 1s 57us/step - loss: 0.1761 - val_loss: 0.1823\n",
      "Epoch 47/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1775 - val_loss: 0.1786\n",
      "Epoch 48/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1774 - val_loss: 0.1784\n",
      "Epoch 49/100\n",
      "16968/16968 [==============================] - 1s 55us/step - loss: 0.1773 - val_loss: 0.1778\n",
      "Epoch 50/100\n",
      "16968/16968 [==============================] - 1s 54us/step - loss: 0.1753 - val_loss: 0.1763\n",
      "Epoch 51/100\n",
      "16968/16968 [==============================] - 1s 56us/step - loss: 0.1754 - val_loss: 0.1823\n",
      "Epoch 52/100\n",
      "16968/16968 [==============================] - 1s 57us/step - loss: 0.1763 - val_loss: 0.1827\n",
      "Epoch 53/100\n",
      "16968/16968 [==============================] - 1s 53us/step - loss: 0.1755 - val_loss: 0.1823\n",
      "Epoch 54/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1754 - val_loss: 0.1797\n",
      "Epoch 55/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1768 - val_loss: 0.1722\n",
      "Epoch 56/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1736 - val_loss: 0.1723\n",
      "Epoch 57/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1756 - val_loss: 0.2151\n",
      "Epoch 58/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1808 - val_loss: 0.2235\n",
      "Epoch 59/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1788 - val_loss: 0.1855\n",
      "Epoch 60/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1736 - val_loss: 0.1815\n",
      "Epoch 61/100\n",
      "16968/16968 [==============================] - 1s 51us/step - loss: 0.1753 - val_loss: 0.1704\n",
      "Epoch 62/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1742 - val_loss: 0.1751\n",
      "Epoch 63/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1713 - val_loss: 0.1641\n",
      "Epoch 64/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1676 - val_loss: 0.1580\n",
      "Epoch 65/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1711 - val_loss: 0.1832\n",
      "Epoch 66/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1737 - val_loss: 0.1842\n",
      "Epoch 67/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1742 - val_loss: 0.1901\n",
      "Epoch 68/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1733 - val_loss: 0.1860\n",
      "Epoch 69/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1720 - val_loss: 0.1742\n",
      "Epoch 70/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1722 - val_loss: 0.1801\n",
      "Epoch 71/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1721 - val_loss: 0.1811\n",
      "Epoch 72/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1705 - val_loss: 0.1848\n",
      "Epoch 73/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1726 - val_loss: 0.1823\n",
      "Epoch 74/100\n",
      "16968/16968 [==============================] - 1s 53us/step - loss: 0.1686 - val_loss: 0.1725\n",
      "Epoch 75/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1688 - val_loss: 0.1762\n",
      "Epoch 76/100\n",
      "16968/16968 [==============================] - 1s 52us/step - loss: 0.1684 - val_loss: 0.1825\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16968/16968 [==============================] - 1s 51us/step - loss: 0.1707 - val_loss: 0.1823\n",
      "Epoch 78/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1697 - val_loss: 0.1648\n",
      "Epoch 79/100\n",
      "16968/16968 [==============================] - 1s 48us/step - loss: 0.1682 - val_loss: 0.1827\n",
      "Epoch 80/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1679 - val_loss: 0.1740\n",
      "Epoch 81/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1729 - val_loss: 0.1686\n",
      "Epoch 82/100\n",
      "16968/16968 [==============================] - 1s 48us/step - loss: 0.1710 - val_loss: 0.1781\n",
      "Epoch 83/100\n",
      "16968/16968 [==============================] - 1s 48us/step - loss: 0.1741 - val_loss: 0.1761\n",
      "Epoch 84/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1698 - val_loss: 0.1830\n",
      "Epoch 85/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1694 - val_loss: 0.1763\n",
      "Epoch 86/100\n",
      "16968/16968 [==============================] - 1s 48us/step - loss: 0.1689 - val_loss: 0.1716\n",
      "Epoch 87/100\n",
      "16968/16968 [==============================] - 1s 48us/step - loss: 0.1655 - val_loss: 0.1704\n",
      "Epoch 88/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1689 - val_loss: 0.1750\n",
      "Epoch 89/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1651 - val_loss: 0.1773\n",
      "Epoch 90/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1692 - val_loss: 0.1686\n",
      "Epoch 91/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1670 - val_loss: 0.1806\n",
      "Epoch 92/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1678 - val_loss: 0.1802\n",
      "Epoch 93/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1725 - val_loss: 0.1839\n",
      "Epoch 94/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1721 - val_loss: 0.1762\n",
      "Epoch 95/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1667 - val_loss: 0.1814\n",
      "Epoch 96/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1696 - val_loss: 0.1816\n",
      "Epoch 97/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1740 - val_loss: 0.2005\n",
      "Epoch 98/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1824 - val_loss: 0.1725\n",
      "Epoch 99/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1689 - val_loss: 0.1572\n",
      "Epoch 100/100\n",
      "16968/16968 [==============================] - 1s 49us/step - loss: 0.1630 - val_loss: 0.1644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b13bcc550>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, \n",
    "          batch_size=batch_size, \n",
    "          epochs=100, \n",
    "          validation_data = (test_X, test_Y),\n",
    "          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calgary doesn't really get that much rain, so there are three naive models I can think of trying and benchmarking against. They are:\n",
    "\n",
    "* simply apply the mean of all previous observations to be the prediction\n",
    "* apply the median of all previous observations\n",
    "* apply the previous observed value as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06843325255505216"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_data['Precip. Binary'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24956727316107954"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def CrossEntropy(yHat, y):\n",
    "    if y == 1:\n",
    "        return -log(yHat)\n",
    "    else:\n",
    "        return -log(1 - yHat)\n",
    "\n",
    "mean_binary_precip = rnn_data['Precip. Binary'].mean()\n",
    "mean_yhat = np.ones(rnn_data.shape[0])*mean_binary_precip\n",
    "mean_crossentropy = pd.DataFrame({'yhat': mean_yhat, 'y': rnn_data['Precip. Binary']})\n",
    "mean_crossentropy['crossEntropy'] = mean_crossentropy.apply(lambda row: CrossEntropy(row['yhat'], row['y']), axis=1)\n",
    "mean_crossentropy['crossEntropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24956727316107954"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_crossentropy['crossEntropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_data['Precip. Binary'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4665844353842155"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_binary_precip = rnn_data['Precip. Binary'].median()\n",
    "median_yhat = np.ones(rnn_data.shape[0])*np.finfo(float).eps\n",
    "median_crossentropy = pd.DataFrame({'yhat': median_yhat, 'y': rnn_data['Precip. Binary']})\n",
    "median_crossentropy['crossEntropy'] = median_crossentropy.apply(lambda row: CrossEntropy(row['yhat'], row['y']), axis=1)\n",
    "median_crossentropy['crossEntropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another naive model that has no timeseries component could be a generalized linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
