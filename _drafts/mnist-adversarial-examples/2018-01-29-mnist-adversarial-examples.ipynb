{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "title: \"Attacking My MNIST Neural Net With Adversarial Examples\"\n",
    "author: \"Everett Robinson\"\n",
    "date: \"January 29, 2018\"\n",
    "output: html_document\n",
    "layout: post\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks appear to be wildly successful at image recognition tasks, but they are far from perfect. They are known to be susceptible to attacks called adversarial examples, in which an image that is clearly of one class to a human observer can be modified in such a way that the neural network misclassifies it. In some cases, the necessary modifications may even be imperceptible to humans.\n",
    "\n",
    "In this post we will explore the topic of adversrial examples using the [Convolutional Neural Network I created for a Kaggle competition](http://everettsprojects.com/2018/01/13/MNIST-CNN.html) and then later [visualized](http://everettsprojects.com/2018/01/17/mnist-visualization.html). To do so I will use the hand drawn digits that the neural network used as a validation set during training, and show that the neural network correctly classifies them 99.74% of the time. I will then use a library called [CleverHans](https://github.com/tensorflow/cleverhans) to compute adversarial examples that cause this accuracy to plummet. Finally I will introduce 10 brand new digits that I have drawn myself and show that they are classified correctly with high confidence. I will then try to compute perturbations that push the model into classifying each of these new example digits into each of the other nine possible digits. These perturbed examples will be visualized to show that the changes required for misclassification are often not as significant as you might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.attacks import BasicIterativeMethod\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "\n",
    "# Set the matplotlib figure size\n",
    "plt.rc('figure', figsize = (12.0, 12.0))\n",
    "\n",
    "# Set the learning phase to false, the model is pre-trained.\n",
    "backend.set_learning_phase(False)\n",
    "keras_model = load_model('models/Jan-13-2018.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Split the provided training data to create a new training\n",
    "data set and a new validation data set. These will be used\n",
    "or hyper-parameter tuning.\n",
    "'''\n",
    "\n",
    "# Use the same seed to get the same validation set\n",
    "seed = 27\n",
    "\n",
    "raw_data = pd.read_csv(\"input/train.csv\")\n",
    "train, validate = train_test_split(raw_data, \n",
    "                                   test_size=0.1,\n",
    "                                   random_state = seed, \n",
    "                                   stratify = raw_data['label'])\n",
    "\n",
    "# Split into input (X) and output (Y) variables\n",
    "x_validation = validate.values[:,1:].reshape(4200,28,28, 1)\n",
    "y_validation = validate.values[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the validation set that was used in training my MNIST Convnet, we can verify that the validation accuracy is actually 99.74% like expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set TF random seed to improve reproducibility\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "if not hasattr(backend, \"tf\"):\n",
    "    raise RuntimeError(\"This tutorial requires keras to be configured\"\n",
    "                       \" to use the TensorFlow backend.\")\n",
    "\n",
    "if keras.backend.image_dim_ordering() != 'tf':\n",
    "    keras.backend.set_image_dim_ordering('tf')\n",
    "    print(\"INFO: '~/.keras/keras.json' sets 'image_dim_ordering' to \"\n",
    "          \"'th', temporarily setting to 'tf'\")\n",
    "\n",
    "# Retrieve the tensorflow session\n",
    "sess =  backend.get_session()\n",
    "\n",
    "# Define input TF placeholder\n",
    "x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "# Evaluate the model's accuracy on the validation data used in training\n",
    "x_validation = x_validation.astype('float32')\n",
    "x_validation /= 255\n",
    "\n",
    "pred = np.argmax(keras_model.predict(x_validation), axis = 1)\n",
    "acc =  np.mean(np.equal(pred, y_validation))\n",
    "\n",
    "print(\"The normal validation accuracy is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see if CleverHans is working. To do so we will initialize the FastGradientMethod attack object, which uses the Fast Gradient Sign Method (FGSM) to generate adversarial examples. The parameters used in this attack are exactly the same as those provided in the [keras tutorial on the CleverHans GitHub Repo](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_tutorial_keras_tf.py). We'll then create the adversarial examples based on the validation data and check the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the Fast Gradient Sign Method (FGSM) attack object and \n",
    "# use it to create adversarial examples as numpy arrays.\n",
    "wrap = KerasModelWrapper(keras_model)\n",
    "fgsm = FastGradientMethod(wrap, sess=sess)\n",
    "fgsm_params = {'eps': 0.3,\n",
    "               'clip_min': 0.,\n",
    "               'clip_max': 1.}\n",
    "adv_x = fgsm.generate_np(x_validation, **fgsm_params)\n",
    "\n",
    "adv_pred = np.argmax(keras_model.predict(adv_x), axis = 1)\n",
    "adv_acc =  np.mean(np.equal(adv_pred, y_validation))\n",
    "\n",
    "print(\"The adversarial validation accuracy is: {}\".format(adv_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy has dropped from a respectable 99.74% to just 20.07% using a FGSM attack. This is one of the simplest adversarial attack methods available, and I expect that better results are possible with more sophisticated methods.\n",
    "\n",
    "To get a feel for what the FGSM attack has done, let's visualize one of the digits beside the corresponding adversarial example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function that stitches the 28 * 28 numpy arrays\n",
    "# together into a collage.\n",
    "def stitch_images(images, y_img_count, x_img_count, margin = 2):\n",
    "    \n",
    "    # Dimensions of the images\n",
    "    img_width = images[0].shape[0]\n",
    "    img_height = images[0].shape[1]\n",
    "    \n",
    "    width = y_img_count * img_width + (y_img_count - 1) * margin\n",
    "    height = x_img_count * img_height + (x_img_count - 1) * margin\n",
    "    stitched_images = np.zeros((width, height, 3))\n",
    "\n",
    "    # Fill the picture with our saved filters\n",
    "    for i in range(y_img_count):\n",
    "        for j in range(x_img_count):\n",
    "            img = images[i * x_img_count + j]\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.dstack([img] * 3)\n",
    "            stitched_images[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                            (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "    return stitched_images\n",
    "\n",
    "x_sample = x_validation[0].reshape(28, 28)\n",
    "adv_x_sample = adv_x[0].reshape(28, 28)\n",
    "\n",
    "adv_comparison = stitch_images([x_sample, adv_x_sample], 1, 2)\n",
    "\n",
    "plt.imshow(adv_comparison)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_digit_img = x_sample.reshape(1, 28, 28, 1)\n",
    "adv_digit_img = adv_x_sample.reshape(1, 28, 28, 1)\n",
    "\n",
    "normal_digit_pred = np.argmax(keras_model.predict(normal_digit_img), axis = 1)\n",
    "adv_digit_pred = np.argmax(keras_model.predict(adv_digit_img), axis = 1)\n",
    "\n",
    "print('The normal digit is predicted to be a {}'.format(normal_digit_pred))\n",
    "print('The adversarial example digit is predicted to be an {}'.format(adv_digit_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attack has perturbed the normal classification from a 7, which is correct, to an 8, which is obviously not. I don't expect that any competent human would mistake the second image for an 8. They may question why there appears to be a bunch of white noise in the image, but if pressed to identify the digit I am confident that nearly everyone will answer that it's a 7.\n",
    "\n",
    "Now let's consider the 10 brand new digits I have created for this exercise. Each of these digits was drawn in Inkscape using a Wacom tablet. I then exported the svg files to a 28 x 28 pixel png image and inverted the colors to get white digits on a black background, just like the original MNIST data my model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the new hand drawn digits from file\n",
    "new_digits = []\n",
    "\n",
    "for i in range(10):\n",
    "    im = imageio.imread('input/new_digits/{}.png'.format(i))\n",
    "    new_digits.append(im[:, :, 0] / 255.)\n",
    "\n",
    "new_digits_img = stitch_images(new_digits, 1, 10)\n",
    "\n",
    "plt.imshow(new_digits_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape the digits to the expected dimensions\n",
    "new_inputs = np.array(new_digits).reshape(10,28,28,1)\n",
    "\n",
    "# Check the accuracy \n",
    "conf = keras_model.predict(new_inputs)\n",
    "pred = np.argmax(conf, axis = 1)\n",
    "acc =  np.mean(np.equal(pred, np.array(range(10))))\n",
    "\n",
    "print(\"The normal classifications are: {}\".format(pred))\n",
    "print(\"The normal classification confidences are: {}\".format(conf.max(axis = 1)))\n",
    "print(\"The normal classification accuracy is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My convnet does a fine job of identifying the new digits, correctly classifying each one with a minimum confidence of 99.88% on the one digit.\n",
    "\n",
    "Now let's see what happens when the examples are perturbed adversarially. This time we will use the Basic Iterative Method for attacks, which is an extension of the FGSM attack that can achieve misclassification with more subtle perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the Basic Iterative Method (BIM) attack object and \n",
    "# use it to create adversarial examples as numpy arrays.\n",
    "bim = BasicIterativeMethod(wrap, sess=sess)\n",
    "bim_params = {'eps_iter': 0.01,\n",
    "              'nb_iter': 100,\n",
    "              'clip_min': 0.,\n",
    "              'clip_max': 1.}\n",
    "adv_x = bim.generate_np(new_inputs, **bim_params)\n",
    "adv_conf = keras_model.predict(adv_x)\n",
    "adv_pred = np.argmax(adv_conf, axis = 1)\n",
    "adv_acc =  np.mean(np.equal(adv_pred, np.array(range(10))))\n",
    "\n",
    "adv_list = np.split(adv_x, list(range(1,10)))\n",
    "adv_list = [img.reshape(28,28) for img in adv_list]\n",
    "adv_img = stitch_images(adv_list, 1, 10)\n",
    "\n",
    "plt.imshow(adv_img, cmap = 'gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"The adversarial classifications are: {}\".format(adv_pred))\n",
    "print(\"The adversarial classification confidences are: {}\".format(adv_conf.max(axis = 1)))\n",
    "print(\"The adversarial classification accuracy is: {}\".format(adv_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy on the adversarial versions of the new digits has dropped to 0% and my convnet is alarmingly confident in these misclassifications. This time the minimum confidence is for the zero digit, which it has predicted is a nine with 99.99982% confidence. Once again, I do not expect any competent human being to make a similar mistake on the above examples.\n",
    "\n",
    "Clearly my MNIST convnet is susceptible to adversarial examples, which isn't surprising given that it was never trained on data that resembles these attacks. It is effectively over-fit to normal looking images of digits that were created in good faith, and adversarial examples expose this over-fitting in a dramatic fashion.\n",
    "\n",
    "An important thing to note, however, is that the above attacks aren't targeted towards a specific misclassification; it simply moves towards the easiest misclassification that it can find. A malicious actor might find more utility in forcing a specific misclassification if they intended to exploit my neural network in the real world. Let's now consider how susceptible my convnet is to targeted adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a label and convert it to a one-hot vector\n",
    "def labels_to_output_layer(labels):\n",
    "    layers = np.zeros((len(labels), 10))\n",
    "    layers[np.arange(len(labels)), labels] = 1\n",
    "    return layers\n",
    "\n",
    "# Take an image in the form of a numpy array and return it with a coloured border\n",
    "def add_border(digit_img, border_color = 'black', margin = 1):\n",
    "    digit_shape = digit_img.shape\n",
    "    base = np.zeros((digit_shape[0] + 2 * margin, digit_shape[1] + 2 * margin, 3))\n",
    "    rgb_digit = np.dstack([digit_img] * 3)\n",
    "    \n",
    "    if border_color == 'red':\n",
    "        base[:,:,0] = 1\n",
    "    elif border_color == 'green':\n",
    "        base[:,:,1] = 1\n",
    "    \n",
    "    border_digit = base\n",
    "    border_digit[margin:(digit_shape[0] + 1), margin:(digit_shape[1] + 1), :] = rgb_digit\n",
    "    \n",
    "    return base\n",
    "\n",
    "# Attempt to perturb the input_digit to be misclassified as each of the target digits\n",
    "# and return the resulting images with colored borders to indicate success or failure\n",
    "def target_attacks(input_digit, input_digit_img, target_digits):\n",
    "    \n",
    "    results = []\n",
    "    bim = BasicIterativeMethod(wrap, sess = sess)\n",
    "    \n",
    "    for target_digit in target_digits:\n",
    "        border_color = 'black'\n",
    "        output_layer = labels_to_output_layer([target_digit])\n",
    "        bim_params = {'eps_iter': 0.01,\n",
    "                      'nb_iter': 1000,\n",
    "                      'y_target': output_layer,\n",
    "                      'clip_min': 0.,\n",
    "                      'clip_max': 1.}\n",
    "        adv_digit = bim.generate_np(input_digit_img, **bim_params)\n",
    "        adv_pred = np.argmax(keras_model.predict(adv_digit), axis = 1)\n",
    "\n",
    "        if adv_pred[0] == input_digit:\n",
    "            border_color = 'green'\n",
    "        elif adv_pred[0] == target_digit:\n",
    "            border_color = 'red'\n",
    "\n",
    "        adv_digit_img = add_border(adv_digit.reshape(28,28), border_color)\n",
    "\n",
    "        results.append(adv_digit_img)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each of the ten digits, attempt to perturb it to the other nine\n",
    "rows = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Make manual garbage collection calls to free up memory\n",
    "    # since my laptop isn't that great\n",
    "    gc.collect()\n",
    "    results = target_attacks(i, new_inputs[i].reshape(1,28,28,1), list(range(10)))\n",
    "    results = [add_border(new_inputs[i])] + results\n",
    "    results_img = stitch_images(results, 1, 11, margin = 0)\n",
    "    rows.append(results_img)\n",
    "    \n",
    "final_img = stitch_images(rows, 10, 1, margin = 0)\n",
    "\n",
    "plt.imshow(final_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column in the above image represents the input digit, and the next ten digits on each row are attempts to perturb it into the digits zero through nine. A green border around a digit indicates that my convnet correctly classified the adversarial example as the original input digit, while a red border means the digit was misclassified as the target digit. The diagonals are all correctly classified since they represent attempts to perturb a digit towards itself. We will not consider these diagonal entries when determining the accuracy of the model.\n",
    "\n",
    "Counting only the green digits off the diagonal, we can see that five of the ninety attacks were correctly classified. Four of these five failed attacks on the digit eight, suggesting that eights are not as easy. Regardless, with 85 out of 90 attacks succeeding, we achieved a 94.4% success rate in forcing specific misclassications. It is no wonder adversarial examples have been such a topic of interest in machine learning circles over the past few years.\n",
    "\n",
    "Now that I know my MNIST convnet is susceptible to adversarial attacks it might be interesting to try a similar technique while treating my network like a black box. To do this I would need to construct a parallel model which is used to find adversarial attacks that are likely to work on the original black box model. Such attacks are [known to work in practice](https://arxiv.org/abs/1605.07277), and there is even a [CleverHans tutorial that implements one](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_blackbox.py). I might also try to improve this model by [incorporating adversarial examples during training](https://arxiv.org/abs/1412.6572). Or I could instead switch to using the new [Capsule Networks](https://www.youtube.com/watch?v=rTawFwUvnLE) introduced only a few months ago by Geoffrey Hinton. Capsule Networks purport to be more resistant to adversarial examples due to the way in which they encode certain features like position, size, orientation, and stroke thickness. It will be interesting to see just how resilient they are against targeted attacks as time passes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
