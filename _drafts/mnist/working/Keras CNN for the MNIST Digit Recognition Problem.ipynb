{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nerual network trained below was created for the [MNIST digit recognition challenge on Kaggle](https://www.kaggle.com/c/digit-recognizer). It produced a public test set accuracy of 0.99700, earning me the 94th spot out of apprimately 2100 entries. This is in the top 5% of scores and well above a line in the rankings at an accuracy of 0.99514 which states \"Something Fishy Going On If You're Above This\". I hope you'll agree that my model isn't doing anything fishy, it just squeezes the data generation techniques available in Keras for everything they are worth.\n",
    "\t\n",
    "\n",
    "# Importing and Splitting the Data into Training and Validation Sets\n",
    "\n",
    "Before diving into the modelling steps, it is necessary to create a new training set and a validation set. This will allow for a fair assessent of how well the model is doing on the more general problem of digit recognition without the effects of overfitting on the training data. It also provides an opportunity to tune various parameters of the neural network such as the batch size, number of epochs for training, data generation strategy, and model architecture. Importing the provided data and then splitting it into the new training and validation sets is done using pandas and scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Split the provided training data to create a new training \n",
    "data set and a new validation data set. These will be used\n",
    "or hyper-parameter tuning.\n",
    "'''\n",
    "\n",
    "# For reproducibility\n",
    "seed = 27\n",
    "\n",
    "raw_data = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "train, validate = train_test_split(raw_data, test_size=0.1, random_state = seed, stratify = raw_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into input (X) and output (Y) variables\n",
    "x_train = train.values[:,1:]\n",
    "y_train = train.values[:,0]\n",
    "\n",
    "x_validate = validate.values[:,1:]\n",
    "y_validate = validate.values[:,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Convolutional Neural Network\n",
    "\n",
    "The model below leverages Keras' built in ImageDataGenerator method with shifts in both dimensions, rotations, shear mapping, and zooming in and out. These randomly distributed transformations are useful to extend the 37,800 training samples we were provided with into a much more diverse training set. By showing the model variations of the original training images, we are also hoping that the neural network will become more robust and less overfit to patterns specific to the data we have. To this end, the model also makes extensve use of dropout, another technique designed to prevent overfitting. The hope is that all of these techniques will allow the neural network to successfully generalize to other digits it hasn't see before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Trains a convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.70% test accuracy after 32 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 512\n",
    "num_classes = 10\n",
    "epochs = 32\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_validate = x_validate.reshape(x_validate.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_validate = x_validate.reshape(x_validate.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_validate = x_validate.astype('float32')\n",
    "x_train /= 255\n",
    "x_validate /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_validate.shape[0], 'validation samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
    "\n",
    "# Use the built in data generation features of Keras\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range = 0.075,\n",
    "    height_shift_range = 0.075,\n",
    "    rotation_range = 12,\n",
    "    shear_range = 0.075,\n",
    "    zoom_range = 0.05,\n",
    "    fill_mode = 'constant',\n",
    "    cval = 0\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (5, 5),\n",
    "                 activation = 'relu',\n",
    "                 input_shape = input_shape))\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    " \n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = keras.losses.categorical_crossentropy,\n",
    "              optimizer = keras.optimizers.Adadelta(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5,\n",
    "                              patience = 2, min_lr = 0.0001)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, \n",
    "                                  y_train, \n",
    "                                  batch_size = batch_size), \n",
    "                    epochs = epochs,\n",
    "                    steps_per_epoch = x_train.shape[0]/32,\n",
    "                    verbose = 1,\n",
    "                    validation_data = (x_validate, y_validate),\n",
    "                    callbacks = [reduce_lr])\n",
    "\n",
    "score = model.evaluate(x_validate, y_validate, verbose = 0)\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions using the test set data\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv(\"../input/test.csv\").values[:,:]\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    test = test.reshape(test.shape[0], 1, img_rows, img_cols)\n",
    "else:\n",
    "    test = test.reshape(test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "test = test.astype('float32')\n",
    "test /= 255\n",
    "\n",
    "pred = np.argmax(model.predict(test), axis = 1)\n",
    "submission = pd.DataFrame(data = pred, columns = ['Label'])\n",
    "submission['ImageId'] = submission.index + 1\n",
    "submission = submission[['ImageId', 'Label']]\n",
    "\n",
    "#submission.to_csv(\"submissions/Jan-13-2018.csv\", index = False)\n",
    "#model.save(\"models/Jan-13-2018.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
